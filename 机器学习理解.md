# 机器学习理解

## 简介

近年来人工智能越来越火，吸引了很多人的注意，人工智能其实是机器学习的一个分支。本文将介绍机器学习的一些理解，了解一下一种分类问题的原理和细节。

## 问题的提出

对机器学习中分类问题的理解，假设现在给你一张图：

![](https://github.com/NGSHotpot/Machine-Learning/blob/master/images/matlab/1.png)

然后让你回答如下问题：（0,0）这个地方如果有一个点，那么这个点会是什么颜色？

在这种情况下，我们会自然地猜想一个答案：红色。因为我们能够直观的感觉到（0,0）这个点离红色的这些点要近，所以它更可能是红色。

## 问题解决

下面我们将这个直观的感觉量化，为计算简单，我们先取只要五个样本点的情形：

![](https://github.com/NGSHotpot/Machine-Learning/blob/master/images/matlab/2.png)

其中一个可行的方案就是计算出所有红色点的重心

![equation](http://latex.codecogs.com/gif.latex?red=\frac{(1,1)+(1,2)+(1,3)}{3}=(1,2))

以及所有蓝色点的重心

![equation](http://latex.codecogs.com/gif.latex?blue=\frac{(7,2)+(9,2)}{2}=(8,2))

然后计算（0,0）点与red的距离

![equation](http://latex.codecogs.com/gif.latex?d_{red}=\sqrt{(1-0)^2+(2-0)^2}=\sqrt{5})

以及（0,0）与blue的距离

![equation](http://latex.codecogs.com/gif.latex?d_{blue}=\sqrt{(8-0)^2+(2-0)^2}=2\sqrt{17})

由于![equation](http://latex.codecogs.com/gif.latex?d_{red}<d_{blue})，由便预测（0,0）点为红色。

另一个可行的方案就是将所有的点都视为具有相同质量1，然后计算（0,0）点所受的万有引力，受红色的点的合力大，则预测为红色，反之，为蓝色。这种量化就相对要麻烦很多。之所以说这个，是想说明我们量化的方式并不唯一，我们可以择优选取。

## 机器学习的建模思路

上面的方法虽然实用，但是处理的方法太过具体，不太容易推广。下面我们介绍对这个问题而言，一般机器学习的建模思路：

### 一、 将问题抽象出来，提炼为带有参数的数学模型。

我们希望用一个参数向量![equation](http://latex.codecogs.com/gif.latex?a=(a_1,a_2,a_3))来作为分类依据：

![equation](http://latex.codecogs.com/gif.latex?x:\left\{\begin{matrix}red,&if{a\cdot\widetilde{x}<0}&\\\\blue,&if{a\cdot\widetilde{x}\geq0}&\end{matrix}\right.)

其中![equation](http://latex.codecogs.com/gif.latex?\widetilde{x}=(x,1))，x为所给数据的坐标，比如 x=(1,3)，则 ![equation](http://latex.codecogs.com/gif.latex?\widetilde{x}=(1,3,1))。

我们还是以这五个点的图为例来说明。为计算简单，我们取参数向量a=(1,0,0)。
在给定a=(1,0,0)的前提下，上述给出的模型的预测效果如下表：



| ![equation](http://latex.codecogs.com/gif.latex?x)         | ![equation](http://latex.codecogs.com/gif.latex?\widetilde{x})           | ![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x})  | 预测颜色 | 真实颜色 |
| :-----------: |:-------------:| :----:| :----:| :----:|
| (1,1)         | (1,1,1)       | 1     | 蓝色  | 红色 |
| (1,2)	        | (1,2,1)	      | 1 	  | 蓝色	| 红色 |
| (1,3)	        | (1,3,1)	      | 1	    | 蓝色	| 红色 |
| (7,2)	        | (7,2,1)	      | 7	    | 蓝色	| 蓝色 |
| (9,2)	        | (9,2,1)	      | 9	    | 蓝色	| 蓝色 |


从预测结果看出，对于参数向量a=(1,0,0)来说，我们的模型效果并不是很好，下面对这个模型的分类效果做出量化评价。

### 二、 设计出一个可以评价参数效果的惩罚函数。

给定一个参数向量以后，我们需要对这个参数向量的分类效果进行评价。具体的想法就是，设计惩罚函数r(x,a)，b(x,a)这个函数满足如下性质：

1. 	当x的标签是红色，且![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x}<0)，这就说明对于这个数据x而言，我们的预测是对的，所以其惩罚可以不计；如果x的标签是红色，而![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x}\geq0)，那么就说明对于这个数据x我们做了错误的预测，需要进行惩罚。一个可行的惩罚函数为

![equation](http://latex.codecogs.com/gif.latex?r(x,a)=\frac{e^{a\cdot\widetilde{x}}}{1+e^{a\cdot\widetilde{x}}})

这个惩罚函数的特点是![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x})越小于0，则r(x,a)取值越小也越接近0，即能表征我们预测得越准确；反之，![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x}) ̃越大于0，则r(x,a)取值越大也越接近1，即能表征我们预测得越离谱。如下图所示：

![](https://github.com/NGSHotpot/Machine-Learning/blob/master/images/matlab/3.png)


2. 当x的标签是蓝色，情况则刚好相反：因为![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x}\geq0)则说明预测准确，于是对应的惩罚函数应该相应取值较小。所以我们可以选取惩罚函数为:

![equation](http://latex.codecogs.com/gif.latex?b(x,a)=\frac{e^{-a\cdot\widetilde{x}}}{1+e^{-a\cdot\widetilde{x}}})

设这些点依次为X={(x1, color1), (x2, color2), ..., (xN, colorN)}，则总的惩罚函数可取为：

![equation](http://latex.codecogs.com/gif.latex?L(X,a)=\sum_{i=1}^{N}{f(x_i,a)})

其中

![equation](http://latex.codecogs.com/gif.latex?f(x_i,a)=\left\{\begin{matrix}r(x_i,a),&color_i=red&\\\\b(x_i,a),&color_i=blue&\end{matrix}\right.)

如上，在已知数据x的前提下，我们可以用L(x,a)函数的值对参数向量a的预测效果进行评估：L(X,a)值越小，说明参数向量a在这组数据X上表现越好。

就这五个数据点的这个例子而言，我们的X={((1,1),红色)((1,2),红色)，((1,3),红色)，((7,2),蓝色)，((9,2),蓝色)},取参数向量a=(1,0,0)，则在这组数据上的惩罚函数为:

![equation](http://latex.codecogs.com/gif.latex?L(X,a)=\sum_{i=1}^{N}{f(x_i,a)}=r(x_1,a)+r(x_2,a)+r(x_3,a)+b(x_4,a)+b(x_5,a))

![equation](http://latex.codecogs.com/gif.latex?L(X,a)=\frac{e^1}{1+e^1}+\frac{e^1}{1+e^1}+\frac{e^1}{1+e^1}+\frac{e^{-7}}{1+e^{-7}}+\frac{e^{-9}}{1+e^{-9}}\approx2.1942)

### 三、利用惩罚函数的参数进行训练。

在给定数据X的前提下，为了找到表现好的参数向量a，也就是L(x,a)的最小值点，我们可以采用梯度下降的方法训练参数。具体的训练方法如下：

* Step1：随机选取初值a（初值不一定随机选取，如果有其他信息，可以给出更好的初值）
* Step2：计算L(x,a)关于a的梯度 ΔL(x,a)=(∂L⁄(∂a_1 ),∂L⁄(∂a_2 ),∂L⁄(∂a_3 ))，其中a=(a_1,a_2,a_3)。
* Step3：更新参数向量a=a- λ∙ΔL(x,a)。其中学习率λ是一个给定的正实数，为学习率。
* Step4：判断是否需要继续更新参数，如是，则转到step2，否则，停止，a即我们所求。

由于每一次的参数更新都使得L(x,a)值变小，即有学习之意。

我们还是以这个具体问题为例，我们看看经过一次参数更新以后L(x,a)的变化。

* Step1：选取初值a=(1,0,0)；
* Step2：计算L(x,a)关于a的梯度 ΔL(x,a)=(∂L⁄(∂a_1 ),∂L⁄(∂a_2 ),∂L⁄(∂a_3 ))=(0.5824,1.1776,0.5888)，
* Step3：更新参数向量a=a- λ∙ΔL(x,a)。其中学习率λ是一个给定的正实数，为学习率。取λ=0.01，则a=a- λ∙ΔL(x,a)=(0.9942,-0.0118,-0.0059)。
* Step4：利用更新后的a=(0.9942,-0.0118,-0.0059)，算得L(X,a)=2.1733。可以看到沿着梯度的反方向，惩罚函数的值确实有所减小。

如此循环三百次，我们可以看到惩罚函数随着学习次数的图如下：

![](https://github.com/NGSHotpot/Machine-Learning/blob/master/images/matlab/3.png)

学习500次后，参数向量a=(1.2085，-1.8361，-1.2396)，在这个参数向量下，模型的预测效果如下：


| ![equation](http://latex.codecogs.com/gif.latex?x)         | ![equation](http://latex.codecogs.com/gif.latex?\widetilde{x})           | ![equation](http://latex.codecogs.com/gif.latex?a\cdot\widetilde{x})  | 预测颜色 | 真实颜色 |
| :-----------: |:-------------:| :----:| :----:| :----:|
| (1,1)         | (1,1,1)       | -1.8672 | 蓝色  | 红色 |
| (1,2)	        | (1,2,1)	      | -3.7033 	  | 蓝色	| 红色 |
| (1,3)	        | (1,3,1)	      | -5.5394    | 蓝色	| 红色 |
| (7,2)	        | (7,2,1)	      | 3.5477    | 蓝色	| 蓝色 |
| (9,2)	        | (9,2,1)	      | 5.9647    | 蓝色	| 蓝色 |


预测颜色与真实颜色完全匹配。

