# 距离及相似性度量

## 简介

相似性度量是用来衡量两个或者多个样本间的相似程度的一种度量，而距离定义的是两个或者多个样本间的差异。这让我们想到两个比较基本的问题，分类和聚类。在分类和聚类问题中，衡量两个样本的相似性或距离就构建分类模型和聚类模型的关键。本文提供了大量的相似性度量和距离计算的方法。

## 分类与聚类

分类问题就是识别出样本的类别，比如医生通过病人症状判断病人得的是什么病就是一个典型的分类问题，实际上医生是通过病人的症状跟已知所以疾病的症状进行相似度衡量来诊断的。聚类分析是将多个样本分成多个类，而在聚类过程实际上是按照样本间的距离来分类的。所谓“物以类聚，人以群分”就是这个道理。


## 距离与相似度量目录

1. 欧氏距离
2. 曼哈顿距离
3. 切比雪夫距离
4. 闵可夫斯基距离
5. 马氏距离
6. 皮尔森相关系数
7. 斯皮尔曼相关系数
8. 偏相关系数
9. 复相关系数
10. 典型相关系数
11. 互信息
12. cosine相似度
13. 简单匹配系数
14. Jaccard 相似度
15. Tonimoto相似度
16. 汉明距离
17. Levenshtein编辑距离
18. Simpson相似度
19. 几何相似度
20. 超几何相似度
21. KL散度

本文涉及到的相似度计算和距离计算我们用两个样本来进行计算，如下

![equation](http://latex.codecogs.com/gif.latex?X=(x_1,x_2,...,x_n))，![equation](http://latex.codecogs.com/gif.latex?Y=(y_1,y_2,...,y_n))


### 1. 欧式距离

欧式距离是最常用，最容易理解的距离，平时我们说的平面上两点，空间中两点的距离就是两点间线段的长度。计算方法如下：

![equation](http://latex.codecogs.com/gif.latex?d=(\sum_{i=1}^{n}{(x_i-y_i)^2})^\frac{1}{2})


### 2. 曼哈顿距离

曼哈顿距离的命名来源于规划方形建筑区块的城市间，最短行车路径而来。比如在曼哈顿，如果忽略掉单向行驶道路及只存在于第三和第十四大道间的斜向车道就是一个方形区块。如下图所示，下图为从左下角到右上角的曼哈顿距离，图中展示了三条路线（红蓝黄），这三条路线距离是相同的，都为曼哈顿距离。曼哈顿距离又叫City Block 距离。

![]()

曼哈顿距离计算如下：

![equation](http://latex.codecogs.com/gif.latex?d=\sum_{i=1}^{n}{\vert{x_i-y_i}\vert})


### 3. 契比雪夫距离

契比雪夫距离定义为两个点的各个坐标数值差的最大值。契比雪夫距离又叫棋牌距离，在国际象棋中，王一步可以走到周围的八个方格中，两个方格的棋牌距离就是王从其中一个方格走到另外一个方格所需要的最小步数。如下图所示。

![]()

契比雪夫距离计算如下：


![equation](http://latex.codecogs.com/gif.latex?d=\lim_{k\rightarrow\infty}(\sum_{i=1}^{n}{\vert{x_i-y_i}\vert^k})^\frac{1}{k})


### 4. 闵可夫斯基距离

闵可夫斯基距离与之前说的三个距离比较类似计算方法如下


![]()

该公式计算出来的就是k阶闵可夫斯基距离。我们注意到当k=2时，就是欧式距离，当k取无穷大时，就是契比雪夫距离，当k=1时，就是曼哈顿距离。

## 阶段小结

从闵可夫斯基距离出发，我们可以得到上述其他的三个距离，那么显然这些距离之间会有一个共性。
大学学过代数的同学可能还记得一个叫范数的东西。向量范数我们可以理解为将向量映射到实数空间，也就是说，向量的范数是一个数，可以比较大小，范数用双竖线表示。

范数要满足以下三个条件

* 非负性：![]()
* 齐次性：![]()
* 三角不等式：![]()

下面我们看一些基本常用的范数

1-范数
‖X‖_1=∑_(i=1)^(i=n)▒|x_i | 
大家仔细看是不是跟曼哈顿距离一样一样的。
2-范数
〖‖X‖_2=(∑_(i=1)^n▒|x_i |^2 )〗^(1/2)
是不是跟欧式距离一样一样的。
无穷-范数
‖X‖_∞=max┬i⁡|x_i |
是不是刚好又和契比雪夫距离一样。
p-范数
〖‖X‖_p=(∑_(i=1)^n▒|x_i |^p )〗^(1/p)
是不是刚好又和闵可夫斯基距离一样。


### 5. 马氏距离

欧式距离在计算个样本间的距离的时候并没有考虑样本各个特性直接的关系，马氏距离基因各个特性间的协方差矩阵来优化欧式距离，能够很好的衡量两个样本的距离。

马氏距离计算方式如下：
d=[(x ⃗-y ⃗ )^T ∑^(-1) (x ⃗-y ⃗ )]^(1/2)
所以我们在计算马氏距离之前是需要计算各个属性间的协方差矩阵的，也就是说，如果只有两个点是没有办法计算马氏距离的。

### 6. 皮尔森相关系数
皮尔森相关系数衡量的是两个样本的相关性，其计算方法为
ρ=cov(X,Y)/(δ_X δ_Y )=(∑_(i=1)^n▒(x_i-x ̅ )(y_i-y ̅ ) )/(√(∑_(i=1)^n▒(x_i-x ̅ )^2 ) √(∑_(i=1)^n▒(y_i-y ̅ )^2 ))
皮尔森相关系数衡量的是两个样本的线性相关性，但是也有其缺点，就是异常值对皮尔森相关系数影响明显。

### 7. 斯皮尔曼相关系数
斯皮尔曼相关系数也就秩相关系数，该相关系数对异常值就不是那么敏感，因为该相关系数是对数据的rank值计算皮尔森相关系数。
斯皮尔曼计算方法如下：
ρ=(∑_(i=1)^n▒(〖Rx〗_i-(Rx) ̅ )(〖Ry〗_i-Ry ̅ ) )/(√(∑_(i=1)^n▒(〖Rx〗_i-(Rx) ̅ )^2 ) √(∑_(i=1)^n▒(〖Ry〗_i-(Ry) ̅ )^2 ))
Rx和Ry为x，y上的排序值。若X = [1.2, 3.1, 2.5], 显然3.1最大，2.5次之，1.2最小，所以Rx = [3, 1, 2]。

### 8. 偏相关系数
计算两个变量的相关性时，消除掉其他变量的影响，那么这两个变量的相关性就是偏相关系数。计算偏相关系数，我们首先要计算各个变量间的协方差矩阵。若总共有N个变量，那么其中的第i个变量与第j个变量的偏相关系数可以由以下公式计算：
ρ_ij=〖∑^(-1)〗_ij/√(〖∑^(-1)〗_ii 〖∑^(-1)〗_jj )
下面给出只有三个变量的情况下其中两个变量的偏相关系数计算方式：
ρ_12(3) =(ρ_12-ρ_13 ρ_23)/(√(1-〖ρ_13〗^2 ) √(1-〖ρ_23〗^2 ))
偏相关系数可以反映两个变量的真实相关性。

### 9. 复相关系数
复相关系数是指一个因变量与两个或两个以上的自变量之间的相关性。大家可能都做过多元线性回归，是的，多元线性回归结果中报告的R-square里面的R就是我们的复相关系数。
在计算复相关过程中，我们可以直接用回归得出，也可以根据回归预测的Y与真实的Y的皮尔森相关系数来计算。
R=(∑_(i=1)^n▒(y_i-y ̅ )(y ̂_i-y ̂  ̅ ) )/(√(∑_(i=1)^n▒(y_i-y ̅ )^2 ) √(∑_(i=1)^n▒(y ̂_i-y ̂  ̅ )^2 ))

### 10. 典型相关系数
典型相关系数是研究多维变量X与多维变量Y相关性的方法。其实在考虑的时候是计算X的某个线性组合与Y的某个线性组合的相关性。加入X为N维，Y为M维，那么就是要找向量a和向量b使得aX与bY的相关性最大，该最大相关性为典型相关系数。
ρ=(a^T ∑_XY b)/(√(a^T ∑_XX a) √(b^T ∑_YY b))



### 11. 互信息
互信息是两个变量间相互依赖性的度量，一般来说，可以理解为互信息度量的是当知道两个变量中的其中一个之后，对另外一个不确定性的减少程度。互信息的计算方式为：
I(X,Y)=∑_(y∈Y)▒∑_(x∈X)▒p(x,y)log(p(x,y)/p(x)p(y) ) 

### 12. cosine相似度
对的，就是我们数学里面学的余弦。高中我们学过余弦，这里扩展到高维空间中，两个向量的余弦可以定义为：
cos(X,Y)=(X∙Y)/‖X‖‖Y‖ 
带入我们向量的值可以得到
cos(X,Y)=(∑_(i=1)^n▒〖x_i∙y_i 〗)/(√(∑_(i=1)^(i=n)▒〖x_i〗^2 ) √(∑_(i=1)^(i=n)▒〖y_i〗^2 ))



### 13. 简单匹配系数
简单匹配系数就是Simple matching coefficient (SMC)，用来检测两个0,1序列的匹配程度，或者说是距离。也就是说，当X,Y只取0或者1时（比如是否有突变）可以用简单匹配系数。
计算方式如下 
SMC=(Number of matching attributes)/(number of attributes)=(M_00+M_11)/(M_00+M_11+M_10+M_01 )
M00代表在X中是0，在Y中也是0的数量
M11代表在X中是1，在Y中也是1的数量
M01代表在X中是0，在Y中也是1的数量
M10代表在X中是1，在Y中也是0的数量

### 14. Jaccard 相似度
想象一下，如果上面我们的SMC中两个变量中0的个数都特别多，那么由于0和1的数量不匹配，当我们想要关注1的时候，大量的0会使得SMC没有效果。
所以就有了Jaccard距离，不考虑00的情况
Jaccard=M_11/(M_11+M_10+M_01 )

### 15. Tonimoto相似度
Tonimoto相似度的思想和Jaccard一样，只是这个时候X,Y中不再是只有0,1值。
先来看计算公式
Tonimoto=∑_(i=1)^(i=n)▒〖x_i∙y_i 〗i=1i=nxi2+i=1i=nyi2-i=1i=nxi∙yi
显然当X,Y中只有0,1时，Tonimoto就退化成了Jaccard系数。



### 16. 汉明距离
有些翻译也叫海明距离。海明距离计算两个字符串、也可以是0，1串从一个字符串变成和另外一个字符串完全相同最少需要替换几个字母的问题。本质来讲，汉明距离就是两个字符串对应位置不相同的个数。


### 17. Levenshtein编辑距离
Levenshtein编辑距离计算两个字符串的距离，这两个字符串长度可以不一样。该距离定义为从一个字符串变换到另外一个字符串最少要进行几次操作。其中一次操作可以改变一个字符的值、在某个位置增加一个字符、删除某个位置的字符。该距离没有直接的公式可以计算，可以用动态规划（Dynamic Programming）的思想编程计算。


### 18. Simpson相似度
Simpson相似度是比较的也是0,1变量的相似性。其计算方式如下
Simpson=M_11/(min⁡{M_10+M_11,M_01+M_11})

### 19. 几何相似度
比较的也是0,1变量的相似性。其计算方式如下
Geometric=〖M_11〗^2/((M_10+M_11 )∙(M_01+M_11 ) )

### 20. 超几何相似度
比较的也是0,1变量的相似性。其计算方式如下
Hypergeometric=-log∑_(i=M_11)^(min⁡{M_10+M_11,M_01+M_11})▒((■(M_10+M_11@i))∙(■(N@M_01+M_11-i)))/((■(N@M_01+M_11 )) )

### 21. KL散度
KL散度又叫相对熵，是用来计算两个概率分布之间的距离。一个概率分布就会有一个熵，两个概率分布熵之间的差异就是KL散度。

熵定义如下：
H(x)=-∫▒p(x)ln(p(x))dx
H(y)=-∫▒q(x)ln(q(x))dx
相对熵定义如下：
KL(x||y)=-∫▒〖p(x)ln p(x)/(q(x)) dx〗
KL(y||x)=-∫▒〖q(x)ln q(x)/(p(x)) dx〗
从上面看出相对熵是相对概念，X相对于Y不等于Y相对于X。



### 总结
上面说了一些距离和相似性度量的方法，有些可能特别常用，而另外一些不这么常用，整理一下供大家参考。	


参考文献
各种维基百科+经验



